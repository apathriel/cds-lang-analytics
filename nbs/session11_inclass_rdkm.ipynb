{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11 - Generative language models for zero-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working with ```FLAN-T5```, a text-to-text model developed by Google. ```FLAN-T5``` is based on ```T5```, which we saw in the lecture, but it has been further finetuned on a range of common text-to-text tasks. This means that it can already perform a lot of the kinds of tasks that people use generative language models for. You can read more in the paper here: [https://arxiv.org/pdf/2210.11416.pdf](https://arxiv.org/pdf/2210.11416.pdf)\n",
    "\n",
    "We load the model from ```huggingface```. We're here using the ```Large``` version, but you can try the other sizes if you want. The ```Large``` version is already 3.4GB, and even larger models will take a long time to download and to run - but they should also see a marked performance improvement.\n",
    " \n",
    "You can read more about the available models here: [https://huggingface.co/docs/transformers/model_doc/flan-t5](https://huggingface.co/docs/transformers/model_doc/flan-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also download and initalize the pretrained tokenizer that fits with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Our goal is to use the knowledge of language that FLAN-T5 has already acquired during training and to use that knowledge in different domains *without any further fine-tuning*. This is an example of what is called *zero-shot* learning.\n",
    "\n",
    "In order for zero-shot learning to be successful, our prompts need to be carefully designed. FLAN-T5 (and similar models) are a bit less flexible than, for example, ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "prompt = \"classify the following text as positive or negative: I absolutely hated this movie\"\n",
    "\n",
    "# translation\n",
    "#prompt = \"translate from English to French: how old are you?\"\n",
    "\n",
    "# question answering\n",
    "#prompt = \"answer the following question: how is cheese made?\"\n",
    "\n",
    "# named entity recognition\n",
    "#prompt = \"find all location entities in this text: Ross comes from Scotland\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass our text prompt to the tokenizer, defing some extra arguments such as the ```max_length``` of our input (anything longer than this will be truncated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, \n",
    "                    max_length = 200,\n",
    "                    return_tensors=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass all of our input prompt tokens to the model and use than to generate an appropriate output from what FLAN-T5 has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs, \n",
    "                            skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this a bit more cleverly by using a single prompt plus a F-string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"classify the following text as positive or negative: {input_text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we could, for example, write functions for specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(input_text:str) -> str:\n",
    "    prompt = f\"classify the following text as positive or negative: {input_text}\"\n",
    "    inputs = tokenizer(prompt, \n",
    "                    max_length = 200,\n",
    "                    return_tensors=\"tf\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "classifier(\"I absolutely hated this movie!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Look through previous notebooks, exercises, and datasets from Language Analytics so far this semester. In small groups, try using either ```Flan-T5``` and **ChatGPT** (or both) try to solve those problems using generative language models.\n",
    "\n",
    "So that would mean, for example:\n",
    "\n",
    "    - Grammatical analysis\n",
    "    - Named entity recognition/extraction\n",
    "    - Classification\n",
    "    - Topic modelling\n",
    "\n",
    "As an illustrative example: try using Flan-T5 to perform classification on the Fake or Real News dataset. How does it perform on ground truth? Is it better or worse than the other models we've seen? Be creative!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd41ab7c459f24986dbaa7</td>\n",
       "      <td>710</td>\n",
       "      <td>By ANDREW E. KRAMER</td>\n",
       "      <td>article</td>\n",
       "      <td>Reporting on Gays Who ‘Don’t Exist’</td>\n",
       "      <td>['Chechnya (Russia)', 'Homosexuality and Bisex...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-24 00:07:04</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>“I see flies, I see mosquitoes,” said a Cheche...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/insider/rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd45a17c459f24986dbaaa</td>\n",
       "      <td>1230</td>\n",
       "      <td>By MATT FLEGENHEIMER and THOMAS KAPLAN</td>\n",
       "      <td>article</td>\n",
       "      <td>The Fights That Could Lead to a Government Shu...</td>\n",
       "      <td>['Trump, Donald J', 'United States Politics an...</td>\n",
       "      <td>3</td>\n",
       "      <td>National</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-04-24 00:23:53</td>\n",
       "      <td>Politics</td>\n",
       "      <td>The Trump administration wants to use the dead...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c2c7c459f24986dbac3</td>\n",
       "      <td>1424</td>\n",
       "      <td>By NOEL MURRAY</td>\n",
       "      <td>article</td>\n",
       "      <td>‘The Leftovers’ Season 3, Episode 2: Swedish P...</td>\n",
       "      <td>['Television', 'The Leftovers (TV Program)']</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-24 02:00:04</td>\n",
       "      <td>Television</td>\n",
       "      <td>For all its melancholy, “The Leftovers” rarely...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/arts/televi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c3d7c459f24986dbac4</td>\n",
       "      <td>1052</td>\n",
       "      <td>By BEN BRANTLEY</td>\n",
       "      <td>article</td>\n",
       "      <td>Thinking Out Loud, But Why?</td>\n",
       "      <td>['Theater', 'The Antipodes (Play)', 'Baker, An...</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-24 02:00:25</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In this endlessly fascinating work, Annie Bake...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/theater/the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>981</td>\n",
       "      <td>By BEN BRANTLEY</td>\n",
       "      <td>article</td>\n",
       "      <td>Some Sugar. Could Use More Spice.</td>\n",
       "      <td>['Theater', 'Charlie and the Chocolate Factory...</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-24 02:00:26</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Christian Borle is the eccentric Willy Wonka i...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/theater/cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abstract                 articleID  articleWordCount  \\\n",
       "0        NaN  58def1347c459f24986d7c80               716   \n",
       "1        NaN  58def3237c459f24986d7c84               823   \n",
       "2        NaN  58def9f57c459f24986d7c90               575   \n",
       "3        NaN  58defd317c459f24986d7c95              1374   \n",
       "4        NaN  58df09b77c459f24986d7ca7               708   \n",
       "..       ...                       ...               ...   \n",
       "881      NaN  58fd41ab7c459f24986dbaa7               710   \n",
       "882      NaN  58fd45a17c459f24986dbaaa              1230   \n",
       "883      NaN  58fd5c2c7c459f24986dbac3              1424   \n",
       "884      NaN  58fd5c3d7c459f24986dbac4              1052   \n",
       "885      NaN  58fd5c3d7c459f24986dbac5               981   \n",
       "\n",
       "                                     byline documentType  \\\n",
       "0       By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                           By GAIL COLLINS      article   \n",
       "2                    By THE EDITORIAL BOARD      article   \n",
       "3                         By MICHAEL POWELL      article   \n",
       "4                              By DEB AMLEN      article   \n",
       "..                                      ...          ...   \n",
       "881                     By ANDREW E. KRAMER      article   \n",
       "882  By MATT FLEGENHEIMER and THOMAS KAPLAN      article   \n",
       "883                          By NOEL MURRAY      article   \n",
       "884                         By BEN BRANTLEY      article   \n",
       "885                         By BEN BRANTLEY      article   \n",
       "\n",
       "                                              headline  \\\n",
       "0    Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                    And Now,  the Dreaded Trump Curse   \n",
       "2                Venezuela’s Descent Into Dictatorship   \n",
       "3                Stain Permeates Basketball Blue Blood   \n",
       "4                            Taking Things for Granted   \n",
       "..                                                 ...   \n",
       "881                Reporting on Gays Who ‘Don’t Exist’   \n",
       "882  The Fights That Could Lead to a Government Shu...   \n",
       "883  ‘The Leftovers’ Season 3, Episode 2: Swedish P...   \n",
       "884                        Thinking Out Loud, But Why?   \n",
       "885                  Some Sugar. Could Use More Spice.   \n",
       "\n",
       "                                              keywords  multimedia    newDesk  \\\n",
       "0    ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1    ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2    ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3    ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                                ['Crossword Puzzles']           3      Games   \n",
       "..                                                 ...         ...        ...   \n",
       "881  ['Chechnya (Russia)', 'Homosexuality and Bisex...           3    Insider   \n",
       "882  ['Trump, Donald J', 'United States Politics an...           3   National   \n",
       "883       ['Television', 'The Leftovers (TV Program)']           3    Culture   \n",
       "884  ['Theater', 'The Antipodes (Play)', 'Baker, An...           3    Culture   \n",
       "885  ['Theater', 'Charlie and the Chocolate Factory...           3    Culture   \n",
       "\n",
       "     printPage              pubDate         sectionName  \\\n",
       "0            2  2017-04-01 00:15:41             Unknown   \n",
       "1           23  2017-04-01 00:23:58             Unknown   \n",
       "2           22  2017-04-01 00:53:06             Unknown   \n",
       "3            1  2017-04-01 01:06:52  College Basketball   \n",
       "4            0  2017-04-01 02:00:14             Unknown   \n",
       "..         ...                  ...                 ...   \n",
       "881          2  2017-04-24 00:07:04             Unknown   \n",
       "882         15  2017-04-24 00:23:53            Politics   \n",
       "883          0  2017-04-24 02:00:04          Television   \n",
       "884          1  2017-04-24 02:00:25             Unknown   \n",
       "885          2  2017-04-24 02:00:26             Unknown   \n",
       "\n",
       "                                               snippet              source  \\\n",
       "0    One of the largest photo displays in Times his...  The New York Times   \n",
       "1                    Meet the gang from under the bus.  The New York Times   \n",
       "2    A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3    For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4    In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "..                                                 ...                 ...   \n",
       "881  “I see flies, I see mosquitoes,” said a Cheche...  The New York Times   \n",
       "882  The Trump administration wants to use the dead...  The New York Times   \n",
       "883  For all its melancholy, “The Leftovers” rarely...  The New York Times   \n",
       "884  In this endlessly fascinating work, Annie Bake...  The New York Times   \n",
       "885  Christian Borle is the eccentric Willy Wonka i...  The New York Times   \n",
       "\n",
       "    typeOfMaterial                                             webURL  \n",
       "0             News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1            Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2        Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3             News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4             News  https://www.nytimes.com/2017/03/31/crosswords/...  \n",
       "..             ...                                                ...  \n",
       "881           News  https://www.nytimes.com/2017/04/23/insider/rus...  \n",
       "882           News  https://www.nytimes.com/2017/04/23/us/politics...  \n",
       "883         Review  https://www.nytimes.com/2017/04/23/arts/televi...  \n",
       "884         Review  https://www.nytimes.com/2017/04/23/theater/the...  \n",
       "885         Review  https://www.nytimes.com/2017/04/23/theater/cha...  \n",
       "\n",
       "[886 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"ArticlesApril2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \n",
      "In the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \n",
      "The word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \n",
      "But that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \n",
      "The FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \n",
      "The FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \n",
      "The covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \n",
      "The New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \n",
      "James Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \n",
      "Countless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \n",
      "Senator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \n",
      "If James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \n",
      "Not too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \n",
      "Either Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \n",
      "The only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \n",
      "And it’s an interesting question. \n",
      "Hillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \n",
      "Pretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \n",
      "There are two possible explanations. \n",
      "Hillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \n",
      "But the other explanation is that her people panicked. \n",
      "Going to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \n",
      "During the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \n",
      "There’s only one reason for such bizarre behavior. \n",
      "The Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \n",
      "Clinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \n",
      "You can smell the fear. \n",
      "The FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \n",
      "The misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \n",
      "The Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \n",
      "The campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \n",
      "Hillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
    "\n",
    "# Access the headline column\n",
    "headline_column = df['text']\n",
    "\n",
    "# Print the headline column\n",
    "print(headline_column[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(input_text:str) -> str:\n",
    "    prompt = f\"classify the following news article as real or fake: {input_text}\"\n",
    "    inputs = tokenizer(prompt, \n",
    "                    max_length = 200,\n",
    "                    return_tensors=\"tf\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE\n"
     ]
    }
   ],
   "source": [
    "y_label = df['label'][0]\n",
    "print(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real']\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier(df['text'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
